{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457d9640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:13.336926Z",
     "iopub.status.busy": "2022-10-19T09:57:13.336317Z",
     "iopub.status.idle": "2022-10-19T09:57:20.069097Z",
     "shell.execute_reply": "2022-10-19T09:57:20.067837Z"
    },
    "papermill": {
     "duration": 6.744278,
     "end_time": "2022-10-19T09:57:20.072036",
     "exception": false,
     "start_time": "2022-10-19T09:57:13.327758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import Input, Conv1D, GlobalMaxPool1D \n",
    "from keras.layers import BatchNormalization, concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.losses import CategoricalCrossentropy, MeanSquaredError\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2514643d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:20.084010Z",
     "iopub.status.busy": "2022-10-19T09:57:20.083377Z",
     "iopub.status.idle": "2022-10-19T09:57:20.088645Z",
     "shell.execute_reply": "2022-10-19T09:57:20.087830Z"
    },
    "papermill": {
     "duration": 0.013348,
     "end_time": "2022-10-19T09:57:20.090612",
     "exception": false,
     "start_time": "2022-10-19T09:57:20.077264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parent\n",
    "INPUT = ROOT/'input'\n",
    "DATA = INPUT/'feedback-prize-english-language-learning'\n",
    "WORK = ROOT/'working'\n",
    "VECS = INPUT/'vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1363ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:20.101927Z",
     "iopub.status.busy": "2022-10-19T09:57:20.101555Z",
     "iopub.status.idle": "2022-10-19T09:57:20.106888Z",
     "shell.execute_reply": "2022-10-19T09:57:20.105834Z"
    },
    "papermill": {
     "duration": 0.013653,
     "end_time": "2022-10-19T09:57:20.109250",
     "exception": false,
     "start_time": "2022-10-19T09:57:20.095597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = ['cohesion', 'syntax', 'vocabulary',\n",
    "              'phraseology', 'grammar', 'conventions']\n",
    "max_len = 1440 #200\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "max_words =30000\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ba63bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:20.121040Z",
     "iopub.status.busy": "2022-10-19T09:57:20.120308Z",
     "iopub.status.idle": "2022-10-19T09:57:20.129267Z",
     "shell.execute_reply": "2022-10-19T09:57:20.128417Z"
    },
    "papermill": {
     "duration": 0.017462,
     "end_time": "2022-10-19T09:57:20.131613",
     "exception": false,
     "start_time": "2022-10-19T09:57:20.114151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decontractions(phrase):\n",
    "    phrase = re.sub(r\"wan\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \"not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def preprocess(text):\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(text.values):\n",
    "        sentence = str(sentence)\n",
    "        sent = sentence.replace('\\n\\n', ' ')\n",
    "        sent = decontractions(sent)\n",
    "        preprocessed.append(sent.lower().strip())\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c595adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:20.143616Z",
     "iopub.status.busy": "2022-10-19T09:57:20.142875Z",
     "iopub.status.idle": "2022-10-19T09:57:20.148513Z",
     "shell.execute_reply": "2022-10-19T09:57:20.147647Z"
    },
    "papermill": {
     "duration": 0.013989,
     "end_time": "2022-10-19T09:57:20.150692",
     "exception": false,
     "start_time": "2022-10-19T09:57:20.136703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mcrmse(y_true, y_pred):\n",
    "    colwise_mse = tf.reduce_mean(tf.square(y_true- y_pred), axis=0)\n",
    "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ad12f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:20.162475Z",
     "iopub.status.busy": "2022-10-19T09:57:20.161765Z",
     "iopub.status.idle": "2022-10-19T09:57:20.171164Z",
     "shell.execute_reply": "2022-10-19T09:57:20.170244Z"
    },
    "papermill": {
     "duration": 0.017908,
     "end_time": "2022-10-19T09:57:20.173554",
     "exception": false,
     "start_time": "2022-10-19T09:57:20.155646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_init(max_len, max_words, embedding_matrix):\n",
    "    input_1 = Input(shape=(max_len,))\n",
    "    embed = Embedding(input_dim=max_words,\n",
    "                      output_dim=128,\n",
    "                      input_length=max_len,\n",
    "                      weights=[embedding_matrix],\n",
    "                      trainable=False)(input_1)\n",
    "\n",
    "    branches = []\n",
    "    x = Dropout(0.2)(embed)\n",
    "    for i in range(2, 6):\n",
    "        branch = Conv1D(128, i,\n",
    "                        padding = 'valid',\n",
    "                        activation = 'relu')(x)\n",
    "        branch = GlobalMaxPool1D()(branch)\n",
    "        branches.append(branch)\n",
    "\n",
    "    x = concatenate(branches, axis=1)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(9)(x)\n",
    "    output = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs = [input_1],\n",
    "                  outputs = [output])\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = mcrmse)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a382b884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:20.186056Z",
     "iopub.status.busy": "2022-10-19T09:57:20.185293Z",
     "iopub.status.idle": "2022-10-19T09:57:20.439463Z",
     "shell.execute_reply": "2022-10-19T09:57:20.438293Z"
    },
    "papermill": {
     "duration": 0.263611,
     "end_time": "2022-10-19T09:57:20.442545",
     "exception": false,
     "start_time": "2022-10-19T09:57:20.178934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  \n",
       "2     3.5         3.0          3.0      3.0          2.5  \n",
       "3     4.5         4.5          4.5      4.0          5.0  \n",
       "4     3.0         3.0          3.0      2.5          2.5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(DATA/'train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed823c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:20.455074Z",
     "iopub.status.busy": "2022-10-19T09:57:20.454097Z",
     "iopub.status.idle": "2022-10-19T09:57:27.907291Z",
     "shell.execute_reply": "2022-10-19T09:57:27.906205Z"
    },
    "papermill": {
     "duration": 7.462185,
     "end_time": "2022-10-19T09:57:27.909980",
     "exception": false,
     "start_time": "2022-10-19T09:57:20.447795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3911/3911 [00:00<00:00, 30236.31it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train.full_text = preprocess(df_train.full_text)\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df_train.full_text)\n",
    "df_train['token'] = token.texts_to_sequences(df_train.full_text)\n",
    "x_train = pad_sequences(df_train.token, maxlen=max_len, padding='post')\n",
    "\n",
    "def extract_vectors(x):\n",
    "    vecs = vec.transform(x)\n",
    "    return vecs.toarray().flatten()\n",
    "\n",
    "vec = TfidfVectorizer(max_features=5000, smooth_idf=True)\n",
    "vec.fit(df_train.full_text)\n",
    "df_train['vec'] = df_train.full_text.apply(lambda x: extract_vectors([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f95f20b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:27.923533Z",
     "iopub.status.busy": "2022-10-19T09:57:27.922514Z",
     "iopub.status.idle": "2022-10-19T09:57:28.248508Z",
     "shell.execute_reply": "2022-10-19T09:57:28.247517Z"
    },
    "papermill": {
     "duration": 0.335548,
     "end_time": "2022-10-19T09:57:28.251209",
     "exception": false,
     "start_time": "2022-10-19T09:57:27.915661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.load(VECS/'embedding.npy')\n",
    "vocab_size = len(token.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa687c1e",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-10-19T09:57:28.264830Z",
     "iopub.status.busy": "2022-10-19T09:57:28.263757Z",
     "iopub.status.idle": "2022-10-19T12:54:12.014859Z",
     "shell.execute_reply": "2022-10-19T12:54:12.013696Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 10604.631129,
     "end_time": "2022-10-19T12:54:12.888065",
     "exception": false,
     "start_time": "2022-10-19T09:57:28.256936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 09:57:28.316181: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-10-19 09:57:28.584452: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 80s 716ms/step - loss: 1.8657 - mcrmse: 0.2435 - val_loss: 1.6972 - val_mcrmse: 0.2341\n",
      "\n",
      "Epoch 00001: val_mcrmse improved from inf to 0.23406, saving model to /kaggle/working/model_cohesion.h5\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 79s 716ms/step - loss: 1.7560 - mcrmse: 0.2394 - val_loss: 1.7171 - val_mcrmse: 0.2360\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.23406\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 79s 715ms/step - loss: 1.7266 - mcrmse: 0.2374 - val_loss: 1.6859 - val_mcrmse: 0.2325\n",
      "\n",
      "Epoch 00003: val_mcrmse improved from 0.23406 to 0.23251, saving model to /kaggle/working/model_cohesion.h5\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 79s 718ms/step - loss: 1.7190 - mcrmse: 0.2375 - val_loss: 1.7117 - val_mcrmse: 0.2351\n",
      "\n",
      "Epoch 00004: val_mcrmse did not improve from 0.23251\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 79s 721ms/step - loss: 1.7192 - mcrmse: 0.2378 - val_loss: 1.7422 - val_mcrmse: 0.2371\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.23251\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 79s 719ms/step - loss: 1.7047 - mcrmse: 0.2365 - val_loss: 1.7267 - val_mcrmse: 0.2368\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.23251\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 80s 729ms/step - loss: 1.7062 - mcrmse: 0.2373 - val_loss: 1.6978 - val_mcrmse: 0.2347\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.23251\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 80s 726ms/step - loss: 1.7059 - mcrmse: 0.2364 - val_loss: 1.6775 - val_mcrmse: 0.2324\n",
      "\n",
      "Epoch 00008: val_mcrmse improved from 0.23251 to 0.23236, saving model to /kaggle/working/model_cohesion.h5\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 80s 725ms/step - loss: 1.7011 - mcrmse: 0.2368 - val_loss: 1.7148 - val_mcrmse: 0.2362\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.23236\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 79s 722ms/step - loss: 1.7036 - mcrmse: 0.2371 - val_loss: 1.7166 - val_mcrmse: 0.2365\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.23236\n",
      "Epoch 1/10\n",
      "98/98 [==============================] - 95s 964ms/step - loss: 1.7039 - mcrmse: 0.2362 - val_loss: 1.7191 - val_mcrmse: 0.2406\n",
      "\n",
      "Epoch 00001: val_mcrmse did not improve from 0.23236\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 95s 965ms/step - loss: 1.7059 - mcrmse: 0.2364 - val_loss: 1.7228 - val_mcrmse: 0.2406\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.23236\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 95s 965ms/step - loss: 1.6974 - mcrmse: 0.2356 - val_loss: 1.7334 - val_mcrmse: 0.2417\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.23236\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 95s 973ms/step - loss: 1.6940 - mcrmse: 0.2360 - val_loss: 1.7343 - val_mcrmse: 0.2412\n",
      "\n",
      "Epoch 00004: val_mcrmse did not improve from 0.23236\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 95s 972ms/step - loss: 1.6888 - mcrmse: 0.2356 - val_loss: 1.7106 - val_mcrmse: 0.2402\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.23236\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 96s 979ms/step - loss: 1.6822 - mcrmse: 0.2352 - val_loss: 1.7227 - val_mcrmse: 0.2415\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.23236\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 94s 963ms/step - loss: 1.6786 - mcrmse: 0.2361 - val_loss: 1.7085 - val_mcrmse: 0.2390\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.23236\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 96s 976ms/step - loss: 1.6787 - mcrmse: 0.2346 - val_loss: 1.7029 - val_mcrmse: 0.2399\n",
      "\n",
      "Epoch 00008: val_mcrmse did not improve from 0.23236\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 95s 968ms/step - loss: 1.6718 - mcrmse: 0.2358 - val_loss: 1.7167 - val_mcrmse: 0.2408\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.23236\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 95s 972ms/step - loss: 1.6600 - mcrmse: 0.2350 - val_loss: 1.7026 - val_mcrmse: 0.2385\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.23236\n",
      "Epoch 1/10\n",
      "110/110 [==============================] - 82s 737ms/step - loss: 1.8862 - mcrmse: 0.2454 - val_loss: 1.7165 - val_mcrmse: 0.2370\n",
      "\n",
      "Epoch 00001: val_mcrmse improved from inf to 0.23697, saving model to /kaggle/working/model_syntax.h5\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 81s 740ms/step - loss: 1.7318 - mcrmse: 0.2368 - val_loss: 1.6598 - val_mcrmse: 0.2304\n",
      "\n",
      "Epoch 00002: val_mcrmse improved from 0.23697 to 0.23043, saving model to /kaggle/working/model_syntax.h5\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 80s 731ms/step - loss: 1.6848 - mcrmse: 0.2338 - val_loss: 1.6953 - val_mcrmse: 0.2340\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.23043\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 81s 737ms/step - loss: 1.6780 - mcrmse: 0.2337 - val_loss: 1.6787 - val_mcrmse: 0.2346\n",
      "\n",
      "Epoch 00004: val_mcrmse did not improve from 0.23043\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 80s 728ms/step - loss: 1.6798 - mcrmse: 0.2337 - val_loss: 1.6586 - val_mcrmse: 0.2326\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.23043\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 80s 732ms/step - loss: 1.6778 - mcrmse: 0.2338 - val_loss: 1.6430 - val_mcrmse: 0.2309\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.23043\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 80s 727ms/step - loss: 1.6745 - mcrmse: 0.2323 - val_loss: 1.6609 - val_mcrmse: 0.2328\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.23043\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 81s 734ms/step - loss: 1.6647 - mcrmse: 0.2331 - val_loss: 1.6335 - val_mcrmse: 0.2301\n",
      "\n",
      "Epoch 00008: val_mcrmse improved from 0.23043 to 0.23014, saving model to /kaggle/working/model_syntax.h5\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 80s 725ms/step - loss: 1.6648 - mcrmse: 0.2335 - val_loss: 1.6300 - val_mcrmse: 0.2300\n",
      "\n",
      "Epoch 00009: val_mcrmse improved from 0.23014 to 0.22999, saving model to /kaggle/working/model_syntax.h5\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 80s 726ms/step - loss: 1.6633 - mcrmse: 0.2321 - val_loss: 1.6355 - val_mcrmse: 0.2303\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.22999\n",
      "Epoch 1/10\n",
      "98/98 [==============================] - 97s 984ms/step - loss: 1.6636 - mcrmse: 0.2322 - val_loss: 1.6873 - val_mcrmse: 0.2356\n",
      "\n",
      "Epoch 00001: val_mcrmse did not improve from 0.22999\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 96s 981ms/step - loss: 1.6545 - mcrmse: 0.2320 - val_loss: 1.6589 - val_mcrmse: 0.2334\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.22999\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 96s 985ms/step - loss: 1.6449 - mcrmse: 0.2313 - val_loss: 1.6283 - val_mcrmse: 0.2297\n",
      "\n",
      "Epoch 00003: val_mcrmse improved from 0.22999 to 0.22968, saving model to /kaggle/working/model_syntax.h5\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 96s 979ms/step - loss: 1.6282 - mcrmse: 0.2310 - val_loss: 1.6133 - val_mcrmse: 0.2291\n",
      "\n",
      "Epoch 00004: val_mcrmse improved from 0.22968 to 0.22915, saving model to /kaggle/working/model_syntax.h5\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 96s 978ms/step - loss: 1.6104 - mcrmse: 0.2291 - val_loss: 1.6129 - val_mcrmse: 0.2286\n",
      "\n",
      "Epoch 00005: val_mcrmse improved from 0.22915 to 0.22855, saving model to /kaggle/working/model_syntax.h5\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 96s 984ms/step - loss: 1.6009 - mcrmse: 0.2292 - val_loss: 1.6332 - val_mcrmse: 0.2330\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.22855\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 96s 981ms/step - loss: 1.5828 - mcrmse: 0.2280 - val_loss: 1.6197 - val_mcrmse: 0.2318\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.22855\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 96s 984ms/step - loss: 1.5467 - mcrmse: 0.2273 - val_loss: 1.5575 - val_mcrmse: 0.2261\n",
      "\n",
      "Epoch 00008: val_mcrmse improved from 0.22855 to 0.22608, saving model to /kaggle/working/model_syntax.h5\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 96s 983ms/step - loss: 1.5299 - mcrmse: 0.2263 - val_loss: 1.6284 - val_mcrmse: 0.2298\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.22608\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 96s 978ms/step - loss: 1.5105 - mcrmse: 0.2261 - val_loss: 1.5567 - val_mcrmse: 0.2281\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.22608\n",
      "Epoch 1/10\n",
      "110/110 [==============================] - 82s 731ms/step - loss: 1.6995 - mcrmse: 0.2282 - val_loss: 1.5575 - val_mcrmse: 0.2200\n",
      "\n",
      "Epoch 00001: val_mcrmse improved from inf to 0.22000, saving model to /kaggle/working/model_vocabulary.h5\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 81s 734ms/step - loss: 1.5858 - mcrmse: 0.2222 - val_loss: 1.5464 - val_mcrmse: 0.2197\n",
      "\n",
      "Epoch 00002: val_mcrmse improved from 0.22000 to 0.21972, saving model to /kaggle/working/model_vocabulary.h5\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 80s 724ms/step - loss: 1.5750 - mcrmse: 0.2218 - val_loss: 1.5657 - val_mcrmse: 0.2212\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.21972\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 80s 732ms/step - loss: 1.5647 - mcrmse: 0.2219 - val_loss: 1.5875 - val_mcrmse: 0.2236\n",
      "\n",
      "Epoch 00004: val_mcrmse did not improve from 0.21972\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 80s 727ms/step - loss: 1.5638 - mcrmse: 0.2199 - val_loss: 1.5547 - val_mcrmse: 0.2189\n",
      "\n",
      "Epoch 00005: val_mcrmse improved from 0.21972 to 0.21891, saving model to /kaggle/working/model_vocabulary.h5\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 81s 735ms/step - loss: 1.5644 - mcrmse: 0.2214 - val_loss: 1.5596 - val_mcrmse: 0.2208\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.21891\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 80s 726ms/step - loss: 1.5668 - mcrmse: 0.2218 - val_loss: 1.5841 - val_mcrmse: 0.2235\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.21891\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 80s 726ms/step - loss: 1.5610 - mcrmse: 0.2208 - val_loss: 1.5718 - val_mcrmse: 0.2223\n",
      "\n",
      "Epoch 00008: val_mcrmse did not improve from 0.21891\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 80s 729ms/step - loss: 1.5593 - mcrmse: 0.2207 - val_loss: 1.5450 - val_mcrmse: 0.2194\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.21891\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 80s 732ms/step - loss: 1.5596 - mcrmse: 0.2206 - val_loss: 1.5500 - val_mcrmse: 0.2200\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.21891\n",
      "Epoch 1/10\n",
      "98/98 [==============================] - 95s 956ms/step - loss: 1.5735 - mcrmse: 0.2216 - val_loss: 1.6166 - val_mcrmse: 0.2258\n",
      "\n",
      "Epoch 00001: val_mcrmse did not improve from 0.21891\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 94s 954ms/step - loss: 1.5664 - mcrmse: 0.2221 - val_loss: 1.5388 - val_mcrmse: 0.2183\n",
      "\n",
      "Epoch 00002: val_mcrmse improved from 0.21891 to 0.21828, saving model to /kaggle/working/model_vocabulary.h5\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 94s 958ms/step - loss: 1.5651 - mcrmse: 0.2211 - val_loss: 1.5420 - val_mcrmse: 0.2191\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.21828\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 94s 961ms/step - loss: 1.5522 - mcrmse: 0.2206 - val_loss: 1.5648 - val_mcrmse: 0.2211\n",
      "\n",
      "Epoch 00004: val_mcrmse did not improve from 0.21828\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 94s 960ms/step - loss: 1.5586 - mcrmse: 0.2207 - val_loss: 1.5383 - val_mcrmse: 0.2188\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.21828\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 94s 955ms/step - loss: 1.5516 - mcrmse: 0.2207 - val_loss: 1.5721 - val_mcrmse: 0.2230\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.21828\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 93s 953ms/step - loss: 1.5373 - mcrmse: 0.2198 - val_loss: 1.5261 - val_mcrmse: 0.2176\n",
      "\n",
      "Epoch 00007: val_mcrmse improved from 0.21828 to 0.21756, saving model to /kaggle/working/model_vocabulary.h5\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 94s 959ms/step - loss: 1.5323 - mcrmse: 0.2205 - val_loss: 1.5285 - val_mcrmse: 0.2190\n",
      "\n",
      "Epoch 00008: val_mcrmse did not improve from 0.21756\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 94s 961ms/step - loss: 1.5114 - mcrmse: 0.2194 - val_loss: 1.5149 - val_mcrmse: 0.2187\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.21756\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 94s 959ms/step - loss: 1.4948 - mcrmse: 0.2186 - val_loss: 1.5234 - val_mcrmse: 0.2213\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.21756\n",
      "Epoch 1/10\n",
      "110/110 [==============================] - 82s 735ms/step - loss: 1.8830 - mcrmse: 0.2427 - val_loss: 1.7120 - val_mcrmse: 0.2351\n",
      "\n",
      "Epoch 00001: val_mcrmse improved from inf to 0.23509, saving model to /kaggle/working/model_phraseology.h5\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 81s 740ms/step - loss: 1.7342 - mcrmse: 0.2359 - val_loss: 1.7101 - val_mcrmse: 0.2357\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.23509\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 81s 733ms/step - loss: 1.7125 - mcrmse: 0.2355 - val_loss: 1.7599 - val_mcrmse: 0.2376\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.23509\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 81s 736ms/step - loss: 1.6782 - mcrmse: 0.2336 - val_loss: 1.7032 - val_mcrmse: 0.2348\n",
      "\n",
      "Epoch 00004: val_mcrmse improved from 0.23509 to 0.23478, saving model to /kaggle/working/model_phraseology.h5\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 81s 733ms/step - loss: 1.6757 - mcrmse: 0.2337 - val_loss: 1.6545 - val_mcrmse: 0.2316\n",
      "\n",
      "Epoch 00005: val_mcrmse improved from 0.23478 to 0.23163, saving model to /kaggle/working/model_phraseology.h5\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 81s 736ms/step - loss: 1.6668 - mcrmse: 0.2333 - val_loss: 1.6578 - val_mcrmse: 0.2319\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.23163\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 81s 733ms/step - loss: 1.6376 - mcrmse: 0.2320 - val_loss: 1.6890 - val_mcrmse: 0.2345\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.23163\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 81s 735ms/step - loss: 1.6244 - mcrmse: 0.2319 - val_loss: 1.6485 - val_mcrmse: 0.2319\n",
      "\n",
      "Epoch 00008: val_mcrmse did not improve from 0.23163\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 80s 732ms/step - loss: 1.6139 - mcrmse: 0.2317 - val_loss: 1.6586 - val_mcrmse: 0.2331\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.23163\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 80s 731ms/step - loss: 1.5941 - mcrmse: 0.2301 - val_loss: 1.5944 - val_mcrmse: 0.2287\n",
      "\n",
      "Epoch 00010: val_mcrmse improved from 0.23163 to 0.22869, saving model to /kaggle/working/model_phraseology.h5\n",
      "Epoch 1/10\n",
      "98/98 [==============================] - 97s 977ms/step - loss: 1.5896 - mcrmse: 0.2304 - val_loss: 1.6357 - val_mcrmse: 0.2336\n",
      "\n",
      "Epoch 00001: val_mcrmse did not improve from 0.22869\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 96s 977ms/step - loss: 1.5746 - mcrmse: 0.2297 - val_loss: 1.6188 - val_mcrmse: 0.2317\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.22869\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 96s 976ms/step - loss: 1.5431 - mcrmse: 0.2282 - val_loss: 1.5905 - val_mcrmse: 0.2305\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.22869\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 96s 979ms/step - loss: 1.5166 - mcrmse: 0.2276 - val_loss: 1.5617 - val_mcrmse: 0.2292\n",
      "\n",
      "Epoch 00004: val_mcrmse did not improve from 0.22869\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 95s 974ms/step - loss: 1.5056 - mcrmse: 0.2265 - val_loss: 1.6122 - val_mcrmse: 0.2326\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.22869\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 96s 975ms/step - loss: 1.4802 - mcrmse: 0.2253 - val_loss: 1.5411 - val_mcrmse: 0.2275\n",
      "\n",
      "Epoch 00006: val_mcrmse improved from 0.22869 to 0.22752, saving model to /kaggle/working/model_phraseology.h5\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 96s 980ms/step - loss: 1.4702 - mcrmse: 0.2252 - val_loss: 1.5223 - val_mcrmse: 0.2272\n",
      "\n",
      "Epoch 00007: val_mcrmse improved from 0.22752 to 0.22717, saving model to /kaggle/working/model_phraseology.h5\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 96s 983ms/step - loss: 1.4399 - mcrmse: 0.2241 - val_loss: 1.6401 - val_mcrmse: 0.2349\n",
      "\n",
      "Epoch 00008: val_mcrmse did not improve from 0.22717\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 95s 975ms/step - loss: 1.4100 - mcrmse: 0.2229 - val_loss: 1.5679 - val_mcrmse: 0.2305\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.22717\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 96s 977ms/step - loss: 1.3808 - mcrmse: 0.2216 - val_loss: 1.5456 - val_mcrmse: 0.2302\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.22717\n",
      "Epoch 1/10\n",
      "110/110 [==============================] - 84s 751ms/step - loss: 1.8423 - mcrmse: 0.2448 - val_loss: 1.7249 - val_mcrmse: 0.2356\n",
      "\n",
      "Epoch 00001: val_mcrmse improved from inf to 0.23559, saving model to /kaggle/working/model_grammar.h5\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 82s 749ms/step - loss: 1.7746 - mcrmse: 0.2416 - val_loss: 1.7840 - val_mcrmse: 0.2401\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.23559\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 82s 745ms/step - loss: 1.7459 - mcrmse: 0.2416 - val_loss: 1.7085 - val_mcrmse: 0.2347\n",
      "\n",
      "Epoch 00003: val_mcrmse improved from 0.23559 to 0.23469, saving model to /kaggle/working/model_grammar.h5\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 82s 746ms/step - loss: 1.7311 - mcrmse: 0.2401 - val_loss: 1.7139 - val_mcrmse: 0.2356\n",
      "\n",
      "Epoch 00004: val_mcrmse did not improve from 0.23469\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 83s 751ms/step - loss: 1.7174 - mcrmse: 0.2393 - val_loss: 1.7395 - val_mcrmse: 0.2376\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.23469\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 83s 758ms/step - loss: 1.6988 - mcrmse: 0.2382 - val_loss: 1.6690 - val_mcrmse: 0.2333\n",
      "\n",
      "Epoch 00006: val_mcrmse improved from 0.23469 to 0.23329, saving model to /kaggle/working/model_grammar.h5\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 83s 759ms/step - loss: 1.6876 - mcrmse: 0.2380 - val_loss: 1.6648 - val_mcrmse: 0.2338\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.23329\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 83s 757ms/step - loss: 1.6676 - mcrmse: 0.2379 - val_loss: 1.6494 - val_mcrmse: 0.2313\n",
      "\n",
      "Epoch 00008: val_mcrmse improved from 0.23329 to 0.23131, saving model to /kaggle/working/model_grammar.h5\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 83s 757ms/step - loss: 1.6625 - mcrmse: 0.2373 - val_loss: 1.6338 - val_mcrmse: 0.2308\n",
      "\n",
      "Epoch 00009: val_mcrmse improved from 0.23131 to 0.23083, saving model to /kaggle/working/model_grammar.h5\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 83s 751ms/step - loss: 1.6478 - mcrmse: 0.2365 - val_loss: 1.6453 - val_mcrmse: 0.2321\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.23083\n",
      "Epoch 1/10\n",
      "98/98 [==============================] - 98s 989ms/step - loss: 1.6487 - mcrmse: 0.2368 - val_loss: 1.6422 - val_mcrmse: 0.2363\n",
      "\n",
      "Epoch 00001: val_mcrmse did not improve from 0.23083\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 98s 997ms/step - loss: 1.6313 - mcrmse: 0.2356 - val_loss: 1.6154 - val_mcrmse: 0.2333\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.23083\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 98s 999ms/step - loss: 1.6045 - mcrmse: 0.2341 - val_loss: 1.6426 - val_mcrmse: 0.2355\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.23083\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 97s 991ms/step - loss: 1.5827 - mcrmse: 0.2341 - val_loss: 1.6248 - val_mcrmse: 0.2346\n",
      "\n",
      "Epoch 00004: val_mcrmse did not improve from 0.23083\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 97s 989ms/step - loss: 1.5574 - mcrmse: 0.2325 - val_loss: 1.7400 - val_mcrmse: 0.2383\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.23083\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 97s 987ms/step - loss: 1.5560 - mcrmse: 0.2323 - val_loss: 1.6184 - val_mcrmse: 0.2350\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.23083\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 97s 987ms/step - loss: 1.5327 - mcrmse: 0.2312 - val_loss: 1.7201 - val_mcrmse: 0.2379\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.23083\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 97s 993ms/step - loss: 1.4984 - mcrmse: 0.2296 - val_loss: 1.6199 - val_mcrmse: 0.2343\n",
      "\n",
      "Epoch 00008: val_mcrmse did not improve from 0.23083\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 97s 988ms/step - loss: 1.4746 - mcrmse: 0.2290 - val_loss: 1.5873 - val_mcrmse: 0.2325\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.23083\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 97s 990ms/step - loss: 1.4473 - mcrmse: 0.2283 - val_loss: 1.6105 - val_mcrmse: 0.2328\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.23083\n",
      "Epoch 1/10\n",
      "110/110 [==============================] - 84s 754ms/step - loss: 1.8324 - mcrmse: 0.2431 - val_loss: 1.7313 - val_mcrmse: 0.2394\n",
      "\n",
      "Epoch 00001: val_mcrmse improved from inf to 0.23943, saving model to /kaggle/working/model_conventions.h5\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 83s 755ms/step - loss: 1.7602 - mcrmse: 0.2396 - val_loss: 1.7433 - val_mcrmse: 0.2405\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.23943\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 82s 749ms/step - loss: 1.7384 - mcrmse: 0.2386 - val_loss: 1.7489 - val_mcrmse: 0.2436\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.23943\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 83s 754ms/step - loss: 1.7199 - mcrmse: 0.2377 - val_loss: 1.6924 - val_mcrmse: 0.2379\n",
      "\n",
      "Epoch 00004: val_mcrmse improved from 0.23943 to 0.23792, saving model to /kaggle/working/model_conventions.h5\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 82s 748ms/step - loss: 1.7155 - mcrmse: 0.2383 - val_loss: 1.7151 - val_mcrmse: 0.2398\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.23792\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 83s 753ms/step - loss: 1.7016 - mcrmse: 0.2377 - val_loss: 1.7057 - val_mcrmse: 0.2391\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.23792\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 82s 744ms/step - loss: 1.6997 - mcrmse: 0.2372 - val_loss: 1.7104 - val_mcrmse: 0.2394\n",
      "\n",
      "Epoch 00007: val_mcrmse did not improve from 0.23792\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 82s 750ms/step - loss: 1.6842 - mcrmse: 0.2360 - val_loss: 1.6798 - val_mcrmse: 0.2379\n",
      "\n",
      "Epoch 00008: val_mcrmse did not improve from 0.23792\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 82s 744ms/step - loss: 1.6808 - mcrmse: 0.2361 - val_loss: 1.6631 - val_mcrmse: 0.2362\n",
      "\n",
      "Epoch 00009: val_mcrmse improved from 0.23792 to 0.23617, saving model to /kaggle/working/model_conventions.h5\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 83s 752ms/step - loss: 1.6650 - mcrmse: 0.2357 - val_loss: 1.6804 - val_mcrmse: 0.2365\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.23617\n",
      "Epoch 1/10\n",
      "98/98 [==============================] - 96s 971ms/step - loss: 1.6885 - mcrmse: 0.2368 - val_loss: 1.6436 - val_mcrmse: 0.2341\n",
      "\n",
      "Epoch 00001: val_mcrmse improved from 0.23617 to 0.23415, saving model to /kaggle/working/model_conventions.h5\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 95s 968ms/step - loss: 1.6681 - mcrmse: 0.2361 - val_loss: 1.7081 - val_mcrmse: 0.2386\n",
      "\n",
      "Epoch 00002: val_mcrmse did not improve from 0.23415\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 95s 968ms/step - loss: 1.6528 - mcrmse: 0.2360 - val_loss: 1.6470 - val_mcrmse: 0.2355\n",
      "\n",
      "Epoch 00003: val_mcrmse did not improve from 0.23415\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 95s 968ms/step - loss: 1.6377 - mcrmse: 0.2351 - val_loss: 1.6181 - val_mcrmse: 0.2333\n",
      "\n",
      "Epoch 00004: val_mcrmse improved from 0.23415 to 0.23333, saving model to /kaggle/working/model_conventions.h5\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 95s 969ms/step - loss: 1.6110 - mcrmse: 0.2339 - val_loss: 1.6103 - val_mcrmse: 0.2335\n",
      "\n",
      "Epoch 00005: val_mcrmse did not improve from 0.23333\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 95s 969ms/step - loss: 1.5866 - mcrmse: 0.2328 - val_loss: 1.6186 - val_mcrmse: 0.2339\n",
      "\n",
      "Epoch 00006: val_mcrmse did not improve from 0.23333\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 95s 970ms/step - loss: 1.5676 - mcrmse: 0.2323 - val_loss: 1.5681 - val_mcrmse: 0.2305\n",
      "\n",
      "Epoch 00007: val_mcrmse improved from 0.23333 to 0.23050, saving model to /kaggle/working/model_conventions.h5\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 95s 969ms/step - loss: 1.5519 - mcrmse: 0.2313 - val_loss: 1.6388 - val_mcrmse: 0.2345\n",
      "\n",
      "Epoch 00008: val_mcrmse did not improve from 0.23050\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 95s 973ms/step - loss: 1.5249 - mcrmse: 0.2302 - val_loss: 1.6070 - val_mcrmse: 0.2361\n",
      "\n",
      "Epoch 00009: val_mcrmse did not improve from 0.23050\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 95s 966ms/step - loss: 1.4966 - mcrmse: 0.2287 - val_loss: 1.5630 - val_mcrmse: 0.2328\n",
      "\n",
      "Epoch 00010: val_mcrmse did not improve from 0.23050\n"
     ]
    }
   ],
   "source": [
    "for col in target_col:\n",
    "    y_train = to_categorical(df_train[col]*2-2, num_classes)\n",
    "    model_save = WORK/f'model_{col}.h5'\n",
    "    checkpoint = ModelCheckpoint(model_save,\n",
    "                                 monitor = 'val_mcrmse',\n",
    "                                 save_best_only = True,\n",
    "                                 verbose = 1)\n",
    "    model = model_init(max_len, vocab_size, embedding_matrix)\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_split = 0.1,\n",
    "                        callbacks = [checkpoint],\n",
    "                        verbose = 1)\n",
    "    model.load_weights(model_save)\n",
    "    model.layers[1].trainable = True\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = mcrmse)\n",
    "    history_2 = model.fit(x_train, y_train,\n",
    "                          batch_size = batch_size,\n",
    "                          epochs = epochs,\n",
    "                          validation_split = 0.2,\n",
    "                          callbacks = [checkpoint],\n",
    "                          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1cef3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T12:54:14.470620Z",
     "iopub.status.busy": "2022-10-19T12:54:14.470207Z",
     "iopub.status.idle": "2022-10-19T12:54:17.689041Z",
     "shell.execute_reply": "2022-10-19T12:54:17.688008Z"
    },
    "papermill": {
     "duration": 4.014605,
     "end_time": "2022-10-19T12:54:17.691689",
     "exception": false,
     "start_time": "2022-10-19T12:54:13.677084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3911it [00:02, 1371.02it/s]\n"
     ]
    }
   ],
   "source": [
    "feats = []\n",
    "y_cols = []\n",
    "for i, row in tqdm(df_train.iterrows()):\n",
    "    feats.append(row.vec)\n",
    "    y_cols.append(row[target_col].astype(float))\n",
    "feats = np.array(feats)\n",
    "y_cols = np.array(y_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35976ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T12:54:19.346562Z",
     "iopub.status.busy": "2022-10-19T12:54:19.345779Z",
     "iopub.status.idle": "2022-10-19T12:54:19.373176Z",
     "shell.execute_reply": "2022-10-19T12:54:19.372379Z"
    },
    "papermill": {
     "duration": 0.819401,
     "end_time": "2022-10-19T12:54:19.375423",
     "exception": false,
     "start_time": "2022-10-19T12:54:18.556022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>when a person has no experience on a job their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>Do you think students would benefit from being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text\n",
       "0  0000C359D63E  when a person has no experience on a job their...\n",
       "1  000BAD50D026  Do you think students would benefit from being...\n",
       "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(DATA/'test.csv')\n",
    "sample = pd.read_csv(DATA/'sample_submission.csv')\n",
    "sample.text_id = test.text_id\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c0a1d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T12:54:21.019466Z",
     "iopub.status.busy": "2022-10-19T12:54:21.018507Z",
     "iopub.status.idle": "2022-10-19T12:54:21.040260Z",
     "shell.execute_reply": "2022-10-19T12:54:21.039128Z"
    },
    "papermill": {
     "duration": 0.806564,
     "end_time": "2022-10-19T12:54:21.042287",
     "exception": false,
     "start_time": "2022-10-19T12:54:20.235723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 7561.85it/s]\n",
      "3it [00:00, 2062.43it/s]\n"
     ]
    }
   ],
   "source": [
    "test.full_text = preprocess(test.full_text)\n",
    "x_test = token.texts_to_sequences(test.full_text)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=max_len)\n",
    "\n",
    "test['vec'] = test.full_text.apply(lambda x: extract_vectors([x]))\n",
    "test_f = []\n",
    "for i, row in tqdm(test.iterrows()):\n",
    "    test_f.append(row.vec)\n",
    "test_f = np.array(test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdaf390b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T12:54:22.624291Z",
     "iopub.status.busy": "2022-10-19T12:54:22.623284Z",
     "iopub.status.idle": "2022-10-19T12:54:22.628366Z",
     "shell.execute_reply": "2022-10-19T12:54:22.627295Z"
    },
    "papermill": {
     "duration": 0.805176,
     "end_time": "2022-10-19T12:54:22.630576",
     "exception": false,
     "start_time": "2022-10-19T12:54:21.825400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = model_init(max_len, vocab_size, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19f461a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T12:54:24.297369Z",
     "iopub.status.busy": "2022-10-19T12:54:24.296631Z",
     "iopub.status.idle": "2022-10-19T12:54:25.016143Z",
     "shell.execute_reply": "2022-10-19T12:54:25.015247Z"
    },
    "papermill": {
     "duration": 1.525769,
     "end_time": "2022-10-19T12:54:25.018621",
     "exception": false,
     "start_time": "2022-10-19T12:54:23.492852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 197ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "def label_transform(pred):\n",
    "    labels = []\n",
    "    for i in range(len(pred)):\n",
    "        max_p = max(pred[i])\n",
    "        for j in range(num_classes):\n",
    "            if max_p == pred[i][j]:\n",
    "                ind = (j + 2) / 2\n",
    "                break\n",
    "        labels.append(ind)\n",
    "    return labels\n",
    "\n",
    "for col in target_col:\n",
    "  #  model_save = VECS/f'model_{col}.h5'\n",
    "    model.load_weights(model_save)\n",
    "    test_pred = model.predict(x_test,\n",
    "                              batch_size = batch_size,\n",
    "                              verbose = 1)\n",
    "    test[f'{col}_cnn'] = label_transform(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b8b2e92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T12:54:26.676849Z",
     "iopub.status.busy": "2022-10-19T12:54:26.675733Z",
     "iopub.status.idle": "2022-10-19T12:59:18.637961Z",
     "shell.execute_reply": "2022-10-19T12:59:18.636771Z"
    },
    "papermill": {
     "duration": 293.598319,
     "end_time": "2022-10-19T12:59:19.484147",
     "exception": false,
     "start_time": "2022-10-19T12:54:25.885828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:51<00:00, 48.66s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(6)):\n",
    "    svr = SVR(C=1)\n",
    "    svr.fit(feats, y_cols[:,i])\n",
    "    pred_s = svr.predict(test_f)\n",
    "    test[f'{target_col[i]}_svr'] = pred_s\n",
    "\n",
    "    ridge = Ridge(alpha=0.1)\n",
    "    ridge.fit(feats, y_cols[:,i])\n",
    "    pred_r = ridge.predict(test_f)\n",
    "    test[f'{target_col[i]}_ridge'] = pred_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9862f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T12:59:21.142263Z",
     "iopub.status.busy": "2022-10-19T12:59:21.141869Z",
     "iopub.status.idle": "2022-10-19T12:59:21.161555Z",
     "shell.execute_reply": "2022-10-19T12:59:21.160390Z"
    },
    "papermill": {
     "duration": 0.808122,
     "end_time": "2022-10-19T12:59:21.163761",
     "exception": false,
     "start_time": "2022-10-19T12:59:20.355639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>2.893385</td>\n",
       "      <td>2.717177</td>\n",
       "      <td>2.983520</td>\n",
       "      <td>2.868692</td>\n",
       "      <td>2.687239</td>\n",
       "      <td>2.806138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>2.859122</td>\n",
       "      <td>2.872375</td>\n",
       "      <td>2.864013</td>\n",
       "      <td>2.744721</td>\n",
       "      <td>2.639232</td>\n",
       "      <td>3.107684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.420841</td>\n",
       "      <td>3.365519</td>\n",
       "      <td>3.551246</td>\n",
       "      <td>3.445748</td>\n",
       "      <td>3.476263</td>\n",
       "      <td>3.412227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0  0000C359D63E  2.893385  2.717177    2.983520     2.868692  2.687239   \n",
       "1  000BAD50D026  2.859122  2.872375    2.864013     2.744721  2.639232   \n",
       "2  00367BB2546B  3.420841  3.365519    3.551246     3.445748  3.476263   \n",
       "\n",
       "   conventions  \n",
       "0     2.806138  \n",
       "1     3.107684  \n",
       "2     3.412227  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in target_col:\n",
    "    sample[col] = (test[f'{col}_cnn'] +\n",
    "                   test[f'{col}_svr'] +\n",
    "                   test[f'{col}_ridge']) / 3\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654ae18f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-19T12:59:22.825024Z",
     "iopub.status.busy": "2022-10-19T12:59:22.823854Z",
     "iopub.status.idle": "2022-10-19T12:59:22.833179Z",
     "shell.execute_reply": "2022-10-19T12:59:22.832231Z"
    },
    "papermill": {
     "duration": 0.806849,
     "end_time": "2022-10-19T12:59:22.835530",
     "exception": false,
     "start_time": "2022-10-19T12:59:22.028681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10941.719137,
   "end_time": "2022-10-19T12:59:26.392604",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-19T09:57:04.673467",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
